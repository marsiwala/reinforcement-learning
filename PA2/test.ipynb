{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from env import EnvSpec, Env, EnvWithModel\n",
    "from policy import Policy\n",
    "\n",
    "from dp import value_iteration, value_prediction\n",
    "from monte_carlo import off_policy_mc_prediction_ordinary_importance_sampling as mc_ois\n",
    "from monte_carlo import off_policy_mc_prediction_weighted_importance_sampling as mc_wis\n",
    "from n_step_bootstrap import off_policy_n_step_sarsa as nsarsa\n",
    "from n_step_bootstrap import on_policy_n_step_td as ntd\n",
    "\n",
    "class RandomPolicy(Policy):\n",
    "    def __init__(self,nA,p=None):\n",
    "        self.p = p if p is not None else np.array([1/nA]*nA)\n",
    "        \n",
    "    def action_prob(self,state,action=None):\n",
    "        return self.p[action]\n",
    "\n",
    "    def action(self,state):\n",
    "        return np.random.choice(len(self.p), p=self.p)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    class OneStateMDP(Env): # MDP introduced at Fig 5.4 in Sutton Book\n",
    "        def __init__(self):\n",
    "            env_spec=EnvSpec(2,2,1.)\n",
    "\n",
    "            super().__init__(env_spec)\n",
    "            self.final_state = 1\n",
    "            self.trans_mat, self.r_mat = self._build_trans_mat()\n",
    "\n",
    "        def _build_trans_mat(self):\n",
    "            trans_mat = np.zeros((2,2,2))\n",
    "\n",
    "            trans_mat[0,0,0] = 0.9\n",
    "            trans_mat[0,0,1] = 0.1\n",
    "            trans_mat[0,1,0] = 0.\n",
    "            trans_mat[0,1,1] = 1.0\n",
    "            trans_mat[1,:,1] = 1.\n",
    "\n",
    "            r_mat = np.zeros((2,2,2))\n",
    "            r_mat[0,0,1] = 1.\n",
    "\n",
    "            return trans_mat, r_mat\n",
    "\n",
    "        def reset(self):\n",
    "            self._state = 0\n",
    "            return self._state\n",
    "\n",
    "        def step(self, action):\n",
    "            assert action in list(range(self.spec.nA)), \"Invalid Action\"\n",
    "            assert self._state != self.final_state, \"Episode has ended!\"\n",
    "\n",
    "            prev_state = self._state\n",
    "            self._state = np.random.choice(self.spec.nS,p=self.trans_mat[self._state,action])\n",
    "            r = self.r_mat[prev_state,action,self._state]\n",
    "\n",
    "            if self._state == self.final_state:\n",
    "                return self._state, r, True\n",
    "            else:\n",
    "                return self._state, r, False\n",
    "\n",
    "    class OneStateMDPWithModel(OneStateMDP,EnvWithModel):\n",
    "        @property\n",
    "        def TD(self) -> np.array:\n",
    "            return self.trans_mat\n",
    "\n",
    "        @property\n",
    "        def R(self) -> np.array:\n",
    "            return self.r_mat\n",
    "\n",
    "    env = OneStateMDP()\n",
    "    env_with_model = OneStateMDPWithModel()\n",
    "\n",
    "    # Test Value Iteration\n",
    "    V_star, pi_star = value_iteration(env_with_model,np.zeros(env_with_model.spec.nS),1e-4)\n",
    "\n",
    "    assert np.allclose(V_star,np.array([1.,0.]),1e-5,1e-2), V_star\n",
    "    assert pi_star.action(0) == 0\n",
    "\n",
    "    eval_policy = pi_star\n",
    "    behavior_policy = RandomPolicy(env.spec.nA)\n",
    "\n",
    "    # Test Value Prediction\n",
    "    V, Q = value_prediction(env_with_model,eval_policy,np.zeros(env.spec.nS),1e-4)\n",
    "    assert np.allclose(V,np.array([1.,0.]),1e-5,1e-2), V\n",
    "    assert np.allclose(Q,np.array([[1.,0.],[0.,0.]]),1e-5,1e-2), Q\n",
    "\n",
    "    V, Q = value_prediction(env_with_model,behavior_policy,np.zeros(env.spec.nS),1e-4)\n",
    "    assert np.allclose(V,np.array([0.1,0.]),1e-5,1e-2), V\n",
    "    assert np.allclose(Q,np.array([[0.19,0.],[0.,0.]]),1e-5,1e-2), Q\n",
    "\n",
    "#     # Gather experience using behavior policy\n",
    "#     N_EPISODES = 100000\n",
    "\n",
    "#     trajs = []\n",
    "#     for _ in tqdm(range(N_EPISODES)):\n",
    "#         states, actions, rewards, done =\\\n",
    "#             [env.reset()], [], [], []\n",
    "\n",
    "#         while not done:\n",
    "#             a = behavior_policy.action(states[-1])\n",
    "#             s, r, done = env.step(a)\n",
    "\n",
    "#             states.append(s)\n",
    "#             actions.append(a)\n",
    "#             rewards.append(r)\n",
    "\n",
    "#         traj = list(zip(states[:-1],actions,rewards,states[1:]))\n",
    "#         trajs.append(traj)\n",
    "\n",
    "#     # On-poilicy evaluation test\n",
    "#     Q_est_ois = mc_ois(env.spec,trajs,behavior_policy,behavior_policy,np.zeros((env.spec.nS,env.spec.nA)))\n",
    "#     Q_est_wis = mc_wis(env.spec,trajs,behavior_policy,behavior_policy,np.zeros((env.spec.nS,env.spec.nA)))\n",
    "# #     V_est_td = ntd(env.spec,trajs,1,0.005,np.zeros((env.spec.nS)))\n",
    "\n",
    "#     assert np.allclose(Q_est_ois,np.array([[0.19,0.],[0.,0.]]),1e-5,1e-1), 'due to stochasticity, this test might fail'\n",
    "#     assert np.allclose(Q_est_wis,np.array([[0.19,0.],[0.,0.]]),1e-5,1e-1), 'due to stochasticity, this test might fail'\n",
    "#     assert np.allclose(Q_est_ois,Q_est_wis), 'Both implementation should be equal in on policy case'\n",
    "# #     assert np.allclose(V_est_td,np.array([0.1,0.]),1e-5,1e-1), 'due to stochasticity, this test might fail'\n",
    "\n",
    "# #     # Off-policy evaluation test\n",
    "#     Q_est_ois = mc_ois(env.spec,trajs,behavior_policy,eval_policy,np.zeros((env.spec.nS,env.spec.nA)))\n",
    "#     Q_est_wis = mc_wis(env.spec,trajs,behavior_policy,eval_policy,np.zeros((env.spec.nS,env.spec.nA)))\n",
    "\n",
    "# #     # Don't panic even though Q_est_ois shows high estimation error. It's expected one!\n",
    "#     print(Q_est_ois)\n",
    "#     print(Q_est_wis)\n",
    "\n",
    "#     # Off-policy SARSA test\n",
    "#     Q_star_est, pi_star_est = nsarsa(env.spec,trajs,behavior_policy,n=1,alpha=0.005,initQ=np.zeros((env.spec.nS,env.spec.nA)))\n",
    "#     assert pi_star_est.action(0) == 0\n",
    "\n",
    "#     # sarsa also could fail to converge because of the similar reason above.\n",
    "#     print(Q_star_est)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9991405, 0.       ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_star.action_prob(1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
