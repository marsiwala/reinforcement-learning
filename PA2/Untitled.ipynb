{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    # Initialize thel value function\n",
    "    V = np.zeros(env.nS)\n",
    "    # While our value function is worse than the threshold theta\n",
    "    while True:\n",
    "        # Keep track of the update done in value function\n",
    "        delta = 0\n",
    "        # For each state, look ahead one step at each possible action and next state\n",
    "        for s in range(env.spec.nS):\n",
    "            v = 0\n",
    "            q = 0\n",
    "            # The possible next actions, policy[s]:[a,action_prob]\n",
    "            for a in range(env.spec.nA): \n",
    "                # For each action, look at the possible next states, \n",
    "                for next_s in range(env.spec.nS): # state transition P[s][a] == [(prob, nextstate, reward, done), ...]\n",
    "                    # Calculate the expected value function\n",
    "                    v += pi.action_prob(s, a) * TD[s, a, next_s] * (R[s, a, next_s] + env.spec.gamma * initV[next_s]) # P[s, a, s']*(R(s,a,s')+Î³V[s'])\n",
    "                    # How much our value function changed across any states .  \n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        # Stop evaluating once our value function update is below a threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from env import EnvSpec, Env, EnvWithModel\n",
    "from policy import Policy\n",
    "\n",
    "from dp import value_iteration, value_prediction\n",
    "from monte_carlo import off_policy_mc_prediction_ordinary_importance_sampling as mc_ois\n",
    "from monte_carlo import off_policy_mc_prediction_weighted_importance_sampling as mc_wis\n",
    "from n_step_bootstrap import off_policy_n_step_sarsa as nsarsa\n",
    "from n_step_bootstrap import on_policy_n_step_td as ntd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.18492603 15.76112267 15.69725322 22.76643343] <dp.value_iteration.<locals>.Vi_policy object at 0x136A5448>\n"
     ]
    }
   ],
   "source": [
    "class OneStateMDP(Env): # MDP introduced at Fig 5.4 in Sutton Book\n",
    "    def __init__(self):\n",
    "        env_spec=EnvSpec(4,3,0.9)\n",
    "\n",
    "        super().__init__(env_spec)\n",
    "        self.final_state = 1\n",
    "        self.p = self.q = .25\n",
    "        self.trans_mat, self.r_mat = self._build_trans_mat()\n",
    "\n",
    "    def _build_trans_mat(self):\n",
    "        trans_mat = np.zeros((4,3,4))\n",
    "\n",
    "        trans_mat[0,1,1] = 1 \n",
    "        trans_mat[1,0,3] = self.p\n",
    "        trans_mat[1,0,1] = 1 - self.p\n",
    "        trans_mat[3,0,0] = 1\n",
    "        \n",
    "        trans_mat[0,2,2] = 1\n",
    "        trans_mat[2,0,0] = 1 - self.q\n",
    "        trans_mat[2,0,3] = self.q\n",
    "\n",
    "        r_mat = np.zeros((4,3,4))\n",
    "        r_mat[2,0,0] = 1\n",
    "        r_mat[2,0,3] = 1\n",
    "        r_mat[3,0,0] = 10\n",
    "        \n",
    "\n",
    "        return trans_mat, r_mat\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = 0\n",
    "        return self._state\n",
    "\n",
    "    def step(self, action):\n",
    "        assert action in list(range(self.spec.nA)), \"Invalid Action\"\n",
    "        assert self._state != self.final_state, \"Episode has ended!\"\n",
    "\n",
    "        prev_state = self._state\n",
    "        self._state = np.random.choice(self.spec.nS,p=self.trans_mat[self._state,action])\n",
    "        r = self.r_mat[prev_state,action,self._state]\n",
    "\n",
    "        if self._state == self.final_state:\n",
    "            return self._state, r, True\n",
    "        else:\n",
    "            return self._state, r, False\n",
    "\n",
    "class OneStateMDPWithModel(OneStateMDP,EnvWithModel):\n",
    "    @property\n",
    "    def TD(self) -> np.array:\n",
    "        return self.trans_mat\n",
    "\n",
    "    @property\n",
    "    def R(self) -> np.array:\n",
    "        return self.r_mat\n",
    "\n",
    "env = OneStateMDP()\n",
    "env_with_model = OneStateMDPWithModel()\n",
    "\n",
    "V_star, pi_star = value_iteration(env_with_model,np.zeros(env_with_model.spec.nS),1e-4)\n",
    "print(V_star, pi_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_star.action(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
